#!/usr/bin/env python3
"""
Quick diagnostic script to test what's accessible on docs.itential.com
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def run_diagnostics():
    """Run diagnostics to see what's accessible."""
    
    print("üîç ITENTIAL DOCS DIAGNOSTICS")
    print("=" * 60)
    
    # Test URLs
    test_urls = [
        "https://docs.itential.com/",
        "https://docs.itential.com/docs",
        "https://docs.itential.com/docs/",
        "https://docs.itential.com/sitemap.xml",
        "https://docs.itential.com/docs/getting-started",
        "https://docs.itential.com/docs/platform-6",
        "https://docs.itential.com/docs/installation"
    ]
    
    print("\n1Ô∏è‚É£ Testing URL accessibility:")
    accessible_urls = []
    
    for url in test_urls:
        try:
            response = requests.head(url, timeout=5, allow_redirects=True)
            status = response.status_code
            if status == 200:
                print(f"  ‚úÖ {url} - Status: {status}")
                accessible_urls.append(url)
            else:
                print(f"  ‚ùå {url} - Status: {status}")
        except Exception as e:
            print(f"  ‚ùå {url} - Error: {e}")
    
    if not accessible_urls:
        print("\n‚ùå No URLs are accessible. Check your internet connection.")
        return
    
    # Test HTML structure
    print("\n2Ô∏è‚É£ Testing HTML structure of main docs page:")
    
    # Try the first accessible URL
    main_url = accessible_urls[0]
    print(f"  Using: {main_url}")
    
    try:
        response = requests.get(main_url, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Check title
        title = soup.find('title')
        if title:
            print(f"  üìÑ Page title: {title.get_text().strip()}")
        
        # Count different types of elements
        print(f"  üîó Total links: {len(soup.find_all('a'))}")
        print(f"  üìù Total headings: {len(soup.find_all(['h1', 'h2', 'h3', 'h4']))}")
        print(f"  üì¶ Total divs: {len(soup.find_all('div'))}")
        
        # Check for common selectors
        selectors = ['main', '.main-content', '.content', 'article', '#content', '.docs-content']
        print("\n  üéØ Checking for content selectors:")
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                print(f"    ‚úÖ Found: {selector}")
            else:
                print(f"    ‚ùå Not found: {selector}")
        
        # Find docs links
        print("\n3Ô∏è‚É£ Looking for /docs/ links:")
        docs_links = []
        for link in soup.find_all('a', href=True)[:100]:  # Check first 100 links
            href = link['href']
            full_url = urljoin(main_url, href)
            
            if '/docs/' in full_url and 'itential.com' in full_url:
                docs_links.append(full_url)
                if len(docs_links) <= 10:
                    print(f"  ‚úÖ {full_url}")
        
        print(f"\n  üìä Found {len(docs_links)} documentation links")
        
        # Test if JavaScript is required
        print("\n4Ô∏è‚É£ Checking if site requires JavaScript:")
        if 'noscript' in str(soup):
            print("  ‚ö†Ô∏è Site may require JavaScript for full functionality")
        
        if 'react' in str(soup).lower() or 'vue' in str(soup).lower() or 'angular' in str(soup).lower():
            print("  ‚ö†Ô∏è Site appears to use JavaScript framework")
        
        # Check for API endpoints
        print("\n5Ô∏è‚É£ Looking for API endpoints in page:")
        page_content = str(soup)
        if '/api/' in page_content:
            print("  ‚úÖ Found API references")
        if 'graphql' in page_content.lower():
            print("  ‚úÖ Found GraphQL references")
        if 'rest' in page_content.lower():
            print("  ‚úÖ Found REST references")
        
    except Exception as e:
        print(f"  ‚ùå Error analyzing page: {e}")
    
    print("\n" + "=" * 60)
    print("üìã DIAGNOSTICS COMPLETE")
    
    if docs_links:
        print("\n‚úÖ Site is accessible and contains documentation links")
        print("üí° Recommendation: Use the fixed scraper (fixed_ultimate_scraper.py)")
    else:
        print("\n‚ö†Ô∏è Site is accessible but no /docs/ links found")
        print("üí° The site structure may have changed or requires JavaScript")
        print("üí° You may need to use Selenium for JavaScript-rendered content")

if __name__ == "__main__":
    run_diagnostics()